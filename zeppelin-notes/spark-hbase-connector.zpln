{
  "paragraphs": [
    {
      "text": "%spark.conf\n\nSPARK_HOME \t/opt/spark-3.1.2\nmaster local[*]\nspark.jars  file:///apps/hostpath/drivers/3.2.1/hbase-spark-1.0.1-SNAPSHOT.jar,file:///opt/hbase-2.4.9/lib/hbase-*.jar\nspark.driver.memory 512M\nspark.executor.instances  2\nspark.executor.memory  512M\n\n# spark.executor.extraClassPath file:///opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/lib/hive-hbase-handler-2.1.1-cdh6.3.2.jar\n#spark.executor.extraLibrary file:///opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/lib/hive-hbase-handler-2.1.1-cdh6.3.2.jar\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-29T16:53:29+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640795692298_1493563315",
      "id": "paragraph_1640795692298_1493563315",
      "dateCreated": "2021-12-29T16:34:52+0000",
      "dateStarted": "2021-12-29T16:53:29+0000",
      "dateFinished": "2021-12-29T16:53:29+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:105"
    },
    {
      "text": "%spark\n\nimport org.apache.hadoop.hbase.spark.HBaseContext\nimport org.apache.hadoop.hbase.HBaseConfiguration\n\nval conf = HBaseConfiguration.create()\nconf.set(\"hbase.zookeeper.quorum\", \"zookeeper:2181\")\nconf.set(\"hbase.zookeeper.property.clientPort\",\"2181\")\nconf.set(\"hbase.spark.config.location\", \"file:///opt/spark-3.1.2/conf/hbase-site.xml\")\n\n// Create HBase context \nnew HBaseContext(spark.sparkContext, conf)\n\n// Create DataFram for Book Table\nval bookDF = spark.read.format(\"org.apache.hadoop.hbase.spark\")\n.option(\"hbase.columns.mapping\",\"title STRING :key, author STRING info:author, year STRING info:year, views STRING analytics:views\")\n.option(\"hbase.table\", \"books\")\n.option(\"hbase.config.resources\", \"file:///opt/spark-3.1.2/conf/hbase-site.xml\") \n.load()\n\nbookDF.printSchema()\nbookDF.show()\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-29T16:53:33+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "title STRING :key, author STRING info:author, year STRING info:year, views STRING analytics:views\nroot\n |-- views: string (nullable = true)\n |-- year: string (nullable = true)\n |-- title: string (nullable = true)\n |-- author: string (nullable = true)\n\n+-----+----+--------------------+------------------+\n|views|year|               title|            author|\n+-----+----+--------------------+------------------+\n|  820|1979| Godel, Escher, Bach|Douglas Hofstadter|\n| 3298|1922|In Search of Lost...|     Marcel Proust|\n+-----+----+--------------------+------------------+\n\nimport org.apache.hadoop.hbase.spark.HBaseContext\nimport org.apache.hadoop.hbase.HBaseConfiguration\n\u001b[1m\u001b[34mconf\u001b[0m: \u001b[1m\u001b[32morg.apache.hadoop.conf.Configuration\u001b[0m = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, hbase-default.xml, hbase-site.xml\n\u001b[1m\u001b[34mbookDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [views: string, year: string ... 2 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=0",
              "$$hashKey": "object:479"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640795934142_1423522912",
      "id": "paragraph_1640795934142_1423522912",
      "dateCreated": "2021-12-29T16:38:54+0000",
      "dateStarted": "2021-12-29T16:53:33+0000",
      "dateFinished": "2021-12-29T16:53:49+0000",
      "status": "FINISHED",
      "$$hashKey": "object:106"
    },
    {
      "text": "%spark\n\ndef uuid = java.util.UUID.randomUUID.toString\nprintln(uuid)",
      "user": "anonymous",
      "dateUpdated": "2021-12-29T16:58:00+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "09b3f816-9905-45da-ad3d-e5c4d4593d4f\n\u001b[1m\u001b[34muuid\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640796047221_1817204418",
      "id": "paragraph_1640796047221_1817204418",
      "dateCreated": "2021-12-29T16:40:47+0000",
      "dateStarted": "2021-12-29T16:58:00+0000",
      "dateFinished": "2021-12-29T16:58:00+0000",
      "status": "FINISHED",
      "$$hashKey": "object:107"
    },
    {
      "text": "%spark\n\ncase class Employee(key: String, fName: String, lName: String,\n                      mName:String, addressLine:String, city:String,\n                      state:String, zipCode:String)\n                      \ndef catalog =\n      s\"\"\"{\n         |\"table\":{\"namespace\":\"default\", \"name\":\"employee\"},\n         |\"rowkey\":\"key\",\n         |\"columns\":{\n         |\"key\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n         |\"fName\":{\"cf\":\"person\", \"col\":\"firstName\", \"type\":\"string\"},\n         |\"lName\":{\"cf\":\"person\", \"col\":\"lastName\", \"type\":\"string\"},\n         |\"mName\":{\"cf\":\"person\", \"col\":\"middleName\", \"type\":\"string\"},\n         |\"addressLine\":{\"cf\":\"address\", \"col\":\"addressLine\", \"type\":\"string\"},\n         |\"city\":{\"cf\":\"address\", \"col\":\"city\", \"type\":\"string\"},\n         |\"state\":{\"cf\":\"address\", \"col\":\"state\", \"type\":\"string\"},\n         |\"zipCode\":{\"cf\":\"address\", \"col\":\"zipCode\", \"type\":\"string\"}\n         |}\n         |}\"\"\".stripMargin\n         ",
      "user": "anonymous",
      "dateUpdated": "2021-12-29T17:00:22+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "defined class Employee\n\u001b[1m\u001b[34mcatalog\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640797080209_2013828793",
      "id": "paragraph_1640797080209_2013828793",
      "dateCreated": "2021-12-29T16:58:00+0000",
      "dateStarted": "2021-12-29T17:00:22+0000",
      "dateFinished": "2021-12-29T17:00:23+0000",
      "status": "FINISHED",
      "$$hashKey": "object:108"
    },
    {
      "text": "%spark\nimport spark.sqlContext.implicits._\nval data = Seq(\n    (\"1\", \"Abby\", \"Smith\", \"K\", \"3456 main\", \"Orlando\", \"FL\", \"45235\"),\n    (\"2\", \"Amaya\", \"Williams\", \"L\", \"123 Orange\", \"Newark\", \"NJ\", \"27656\"),\n    (\"3\", \"Alchemy\", \"Davis\", \"P\", \"Warners\", \"Sanjose\", \"CA\", \"34789\")\n)\nval dataDF = data.toDF(Seq(\"key\",\"fName\",\"lName\",\"mName\", \"addressLine\", \"city\", \"state\", \"zipCode\"):_*)",
      "user": "anonymous",
      "dateUpdated": "2021-12-29T17:16:17+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import spark.sqlContext.implicits._\n\u001b[1m\u001b[34mdata\u001b[0m: \u001b[1m\u001b[32mSeq[(String, String, String, String, String, String, String, String)]\u001b[0m = List((1,Abby,Smith,K,3456 main,Orlando,FL,45235), (2,Amaya,Williams,L,123 Orange,Newark,NJ,27656), (3,Alchemy,Davis,P,Warners,Sanjose,CA,34789))\n\u001b[1m\u001b[34mdataDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [key: string, fName: string ... 6 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640797222899_90835071",
      "id": "paragraph_1640797222899_90835071",
      "dateCreated": "2021-12-29T17:00:22+0000",
      "dateStarted": "2021-12-29T17:16:17+0000",
      "dateFinished": "2021-12-29T17:16:18+0000",
      "status": "ABORT",
      "$$hashKey": "object:109"
    },
    {
      "title": "Write To Hbase Table",
      "text": "%spark\n\nimport org.apache.hadoop.hbase.spark.datasources._\n\ndataDF.write.option(\"hbase.config.resources\", \"file:///opt/spark-3.1.2/conf/hbase-site.xml\")\n.option(\"hbase.spark.config.location\", \"/opt/sandbox/spark-2.3.4/conf\")\n.options(Map(HBaseTableCatalog.tableCatalog -> catalog, HBaseTableCatalog.newTable -> \"4\"))\n.format(\"org.apache.hadoop.hbase.spark\")\n.save()\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-29T17:23:23+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.hadoop.hbase.spark.datasources._\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=2",
              "$$hashKey": "object:645"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640797304465_1102188110",
      "id": "paragraph_1640797304465_1102188110",
      "dateCreated": "2021-12-29T17:01:44+0000",
      "dateStarted": "2021-12-29T17:16:35+0000",
      "dateFinished": "2021-12-29T17:16:36+0000",
      "status": "FINISHED",
      "$$hashKey": "object:110"
    },
    {
      "title": "Read Hbase Table",
      "text": "%spark\n\n\nimport spark.implicits._\n\n// Reading from HBase to DataFrame\nval hbaseDF = spark.read\n  .options(Map(HBaseTableCatalog.tableCatalog -> catalog))\n  .format(\"org.apache.hadoop.hbase.spark\")\n  .load()\n\n//Display Schema from DataFrame\nhbaseDF.printSchema()\n\n//Collect and show Data from DataFrame\nhbaseDF.show(false)\n\n//Applying Filters\nhbaseDF.filter($\"key\" === \"1\" && $\"state\" === \"FL\")\n  .select(\"key\", \"fName\", \"lName\")\n  .show()\n      \n      \n      \n",
      "user": "anonymous",
      "dateUpdated": "2021-12-29T17:23:54+0000",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- addressLine: string (nullable = true)\n |-- city: string (nullable = true)\n |-- lName: string (nullable = true)\n |-- key: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipCode: string (nullable = true)\n |-- mName: string (nullable = true)\n |-- fName: string (nullable = true)\n\n+-----------+-------+--------+---+-----+-------+-----+-------+\n|addressLine|city   |lName   |key|state|zipCode|mName|fName  |\n+-----------+-------+--------+---+-----+-------+-----+-------+\n|3456 main  |Orlando|Smith   |1  |FL   |45235  |K    |Abby   |\n|123 Orange |Newark |Williams|2  |NJ   |27656  |L    |Amaya  |\n|Warners    |Sanjose|Davis   |3  |CA   |34789  |P    |Alchemy|\n+-----------+-------+--------+---+-----+-------+-----+-------+\n\njava.lang.NoClassDefFoundError: org/apache/hadoop/hbase/spark/protobuf/generated/SparkFilterProtos$SQLPredicatePushDownFilter\n  at org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter.toByteArray(SparkSQLPushDownFilter.java:250)\n  at org.apache.hadoop.hbase.spark.datasources.SerializedFilter$.$anonfun$toSerializedTypedFilter$1(HBaseTableScanRDD.scala:273)\n  at scala.Option.map(Option.scala:230)\n  at org.apache.hadoop.hbase.spark.datasources.SerializedFilter$.toSerializedTypedFilter(HBaseTableScanRDD.scala:273)\n  at org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.$anonfun$getPartitions$2(HBaseTableScanRDD.scala:85)\n  at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n  at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n  at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n  at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n  at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n  at org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.getPartitions(HBaseTableScanRDD.scala:77)\n  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n  at scala.Option.getOrElse(Option.scala:189)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n  at scala.Option.getOrElse(Option.scala:189)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n  at org.apache.hadoop.hbase.spark.HBaseRelation.buildScan(DefaultSource.scala:369)\n  at org.apache.spark.sql.execution.datasources.DataSourceStrategy$.$anonfun$apply$4(DataSourceStrategy.scala:332)\n  at org.apache.spark.sql.execution.datasources.DataSourceStrategy$.$anonfun$pruneFilterProject$1(DataSourceStrategy.scala:365)\n  at org.apache.spark.sql.execution.datasources.DataSourceStrategy$.pruneFilterProjectRaw(DataSourceStrategy.scala:442)\n  at org.apache.spark.sql.execution.datasources.DataSourceStrategy$.pruneFilterProject(DataSourceStrategy.scala:364)\n  at org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:332)\n  at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n  at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n  at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n  at org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n  at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n  at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:162)\n  at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:162)\n  at scala.collection.Iterator.foreach(Iterator.scala:941)\n  at scala.collection.Iterator.foreach$(Iterator.scala:941)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n  at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n  at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n  at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n  at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n  at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n  at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n  at org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n  at org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:391)\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:104)\n  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:104)\n  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:97)\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:117)\n  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:117)\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:110)\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$simpleString$2(QueryExecution.scala:161)\n  at org.apache.spark.sql.execution.ExplainUtils$.processPlan(ExplainUtils.scala:115)\n  at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:161)\n  at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:206)\n  at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:175)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:98)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:825)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:784)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:793)\n  ... 48 elided\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=3",
              "$$hashKey": "object:691"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=4",
              "$$hashKey": "object:692"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=5",
              "$$hashKey": "object:693"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640797699892_2145210380",
      "id": "paragraph_1640797699892_2145210380",
      "dateCreated": "2021-12-29T17:08:19+0000",
      "dateStarted": "2021-12-29T17:21:16+0000",
      "dateFinished": "2021-12-29T17:21:17+0000",
      "status": "ABORT",
      "$$hashKey": "object:111"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-29T17:21:16+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640798476978_174538416",
      "id": "paragraph_1640798476978_174538416",
      "dateCreated": "2021-12-29T17:21:16+0000",
      "status": "READY",
      "$$hashKey": "object:112"
    }
  ],
  "name": "spark-hbase-connector",
  "id": "2GRH9CDWB",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/hbase/spark-hbase-connector"
}