{"cells":[{"cell_type":"code","source":["# To avoid un-necessary computation across the default 200 partitions\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", 1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql.functions import from_json, col\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["kafkaHost = \"ec2-52-66-45-236.ap-south-1.compute.amazonaws.com:9092\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["linesSchema = StructType([StructField(\"ts\", LongType(), True),\n                         StructField(\"content\", StringType(), True)])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["1. Load the dataframe from kafka topic\n2. Read the key and the value and cast them\n3. Extract the content and the timestamp from json"],"metadata":{}},{"cell_type":"code","source":["lines = spark.readStream\\\n  .format(\"kafka\")\\\n  .option(\"kafka.bootstrap.servers\", kafkaHost)\\\n  .option(\"subscribe\", \"lines\")\\\n  .option(\"startingOffsets\", \"earliest\")\\\n  .load()\\\n  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n  .select(from_json(col(\"value\"), linesSchema).alias(\"lines\"))\\\n  .selectExpr(\"lines.content AS content\", \"CAST(lines.ts AS TIMESTAMP) AS timestamp\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.functions import explode, split"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["1. Do the word count\n2. Apply the watermark of 5 seconds"],"metadata":{}},{"cell_type":"code","source":["# Split the lines into words\nwords = lines.select(\n    # explode turns each item in an array into a separate row\n    explode(split(lines.content, ' ')).alias('word'),\n    lines.timestamp).\\\n  withWatermark(\"timestamp\", \"5 seconds\")\n\nwords.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- word: string (nullable = true)\n-- timestamp: timestamp (nullable = true)\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql.functions import window"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Create the window of 10 seconds sliding every 5 seconds"],"metadata":{}},{"cell_type":"code","source":["# Generate running word count\nwindowedCounts = words.groupBy(\n                       window(words.timestamp, \"10 seconds\", \"5 seconds\"),\n                       words.word).\\\n                count()\n\nwindowedCounts.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- window: struct (nullable = true)\n    |-- start: timestamp (nullable = true)\n    |-- end: timestamp (nullable = true)\n-- word: string (nullable = true)\n-- count: long (nullable = false)\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["checkpointDir = \"dbfs:///tmp/checkpoint/kafkawordcount\"\ndbutils.fs.rm(checkpointDir, True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: False</div>"]}}],"execution_count":13},{"cell_type":"code","source":["query = windowedCounts \\\n    .writeStream \\\n    .outputMode(\"append\") \\\n    .option(\"truncate\", \"false\") \\\n    .option(\"checkpointLocation\", checkpointDir) \\\n    .format(\"console\") \\\n    .queryName(\"kafkawordcount\") \\\n    .start()\n\nquery.awaitTermination()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Let these be the first set of inputs <br> <br>\n{\"ts\": 1581098400, \"content\": \"apple orange\"} <br>\n{\"ts\": 1581098403, \"content\": \"melon pear\"} <br>\n{\"ts\": 1581098406, \"content\": \"apple orange\"} <br>\n{\"ts\": 1581098409, \"content\": \"melon pear\"} <br>\n{\"ts\": 1581098412, \"content\": \"apple orange\"} <br>\n{\"ts\": 1581098415, \"content\": \"melon pear\"} <br>\n***\nIf the max ts of message is M <br>\nand late threshold is L <br>\nthen window till (M - L) is finalized <br> <br>\nSo far the window till 18:00:10 should have been finalized (also the watermark is 1581098415 - 5 == 18:00:10) <br> <br>\nExpected windows in the output: <br>\n- 17:59:55 - 18:00:05 <br>\n- 18:00:00 - 18:00:10 <br>\n***\nWindow till 18:00:15 will be finalized when watermark moves there or crosses it\n***\nBelow records will have no impact <br>\n{\"ts\": 1581098417, \"content\": \"apple orange\"} <br>\n{\"ts\": 1581098419, \"content\": \"melon pear\"} <br>\n***\nAlso, this record will have no impact on the word count of window 18:00:00 - 18:00:10 (as it is before the watermark) <br>\nThis will only affect the window 18:00:05 - 18:00:15 (once that window is finalized) <br>\nTODO: Check if this has any impact, it should be discarded being late than the threshold<br>\n{\"ts\": 1581098409, \"content\": \"melon pear\"} <br>\n***\nBut this record will cause the finalization <br>\n{\"ts\": 1581098420, \"content\": \"apple orange\"} <br>\n***\nNew window: <br>\n18:00:05 - 18:00:15"],"metadata":{}}],"metadata":{"name":"Stream from Kafka","notebookId":2942505623269092},"nbformat":4,"nbformat_minor":0}
